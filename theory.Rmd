# Theory

When you're first learning R, getting on R and planning little projects and writing code is definitely the best way to learn. Reading to understand *why* you're getting the output that you are or why you're doing something the way you are doing is definitely important, but it's always better to get hands on.

Having said that, one thing that I craved when I was learning R was to understand why people coded the way they did, or why one thing was always recommended over another in StackOverflow answers. I picked it up along the way, but there were many times where I was doing something completely unnecessary or inefficiently because I hadn't been exposed to a discussion about why I shouldn't be doing what I was doing. Similarly, when I eventually did come across an article outlining some of the philosophy or theory underpinning an approach, a little light switch would go and so many more things would click into place.

So this chapter is dedicated purely to some of the simple theory underpinning certain actions in R. This is an *opinionated* piece as I hold a personal opinion on how certain things should be done in R, but that doesn't mean that I'm right. Instead, I hope this section helps you think more deeply about what you're trying to achieve and the best way to get there before you start your next project.

## Abstraction

For me, the biggest change in the quality and efficiency of my code was when I began learning about the concept of abstraction. Abstraction is essentially the process of breaking down complex processes and objects into their base function or quality. From here, we can abstract away levels of complexity, thus creating cleaner looking code.

Abstraction is relevant to every aspect of your projects, from how they are structured to your functions and code and so we're going to spend some time understanding abstraction and how it can be applied.

### Definition

Abstraction is the idea of removing levels of complexity. For example, when you press a key on your keyboard and a letter appears on the screen, you don't need to know how the keyboard interfaces with the computer, or how that stroke is eventually turned into coloured pixels on a screen. That degree of complexity has been **abstracted** away.

Another example is a calculator. You type in the numbers and decide what you want to do, and your general goal (say, adding two numbers together) is translated into the practicality of performing that action. Your general goal is translated into lots of little more specific ones.

The idea of abstraction is a very prevalent one in computer science. R itself is an abstraction; it lets you interface with the CPU without having to know everything about it. Understanding abstraction and particularly how it relates to functional programming and R can greatly improve the efficiency of your code. Understanding and applying abstraction is more of an art than a science. By abstracting away complexity, you make things easier for the user but you will take away some of the flexability, and so applying the concept of abstraction to your projects will always be a balancing act.

### Abstraction in R

Finding examples of abstraction in existing R code is easy. For instance, in the [teacheR](https://teacher.arawles.co.uk) book, we looked at how R used method dispatch to find the appropriate method for a particular type of object. If you print a dataframe, for instance, then R will find the appropriate method to print that type of object (a dataframe), and will eventually call the `print.data.frame()` function to do so. But that's not what you have to type in. You just type `print(my_dataframe)` and R takes care of the rest. And that's a good example of how R has abstracted away that complexity, instead focusing on the core goal - printing something.

Applying abstraction to your own R code can greatly improve your code cleanliness and efficiency. One of our main tools for implementing abstraction into our projects are functions. We want to break down our steps into their smallest constituent parts, and aim to create functions that are as simple as possible. We can then use these functions together to solve the overarching issue. The best way to demonstrate how powerful abstraction can be in R is probably through an example. First we'll look at a simple example and then we'll move onto something more complex.

#### Example 1 - `plotly` labels

This is a real example of abstraction that I used very recently. The `plotly` package is a great library for producing interactive graphs for Shiny applications and RMarkdown documents. I often use the `ggplotly()` function, which takes a `ggplot` object and converts it to a plotly one. 

One of the features of the package is that you can create a tooltip, such that the individual looking at the graph can hover their cursor over the line or bar or whatever and a little box will pop up showing them the value that they're hovering over.

The tooltips are generated automatically from the aesthetics you define in your `ggplot2` call. So if you create a graph with an `x` called `x_val`, then the tooltip will say something like "x_val: 100" when you hover over it. This is fine, but it can look a bit messy when you don't have very nice variable names, which I often don't. Instead, you can create a new aesthetic (like `text`) and provide your values to that aesthetic with nicer names. For example:

```{r, eval = FALSE}
gplot <- ggplot(data, aes(x = x_val, y = y_val, text = paste0("Better X Label:" = x)))...
plotly_plot <- ggplotly(gplot, tooltip = "text")
```

Now, the tooltip will show "Better X Label: 100" instead of "x: 100". Much better.

But what about when you want to have more than one variable in the label? You could try adding more aesthetics and including them to the `tooltip` parameter (`tooltip = c("text", "label", etc.)`), but that could be tough if you've got more than a couple of aesthetics.

So let's think less about the here and now and try and break this into the simplest possible terms. We want a function that will need to accept a new name for our variable, the variable values, and it will need to be able to accept any number of variables. After a bit of trial and error, this is the function I created:

```{r}
plotly_label <- function(labels) {
  raw_lists <- purrr::map2(names(labels), labels, function(name, values) {
    purrr::map(values, ~paste0(name, ": ", .x))
  })
  purrr::pmap(raw_lists, function(...) paste(..., sep = "<br>"))
}
```

This function accepts a list of name-value pairs, with the name representing the new name of the variable. This function then returns new labels (separated by a break) that can be used by `plotly`. So for instance, if I wanted to have labels for both my `x` and `y` variables, I could just do this:

```{r, eval = FALSE}
gplot <- ggplot(data, aes(x = x_val, y = y_val, text = plotly_label(list("Better X Label:" = x, "Better Y Label:" = y))))...
plotly_plot <- ggplotly(gplot, tooltip = "text")
```

We haven't changed the world here, but we have created a function that can be used in multiple situations. We haven't hard-coded anything in, meaning that theoretically we could create a tooltip with thousands of variables.

#### Example 2 - Plotting

For our first example, let's imagine you want to create a full suite of graphics for a reporting project you're working on. You're going to want to create 10 different line charts and 5 different bar charts using different variables from 3 different datasets. Let's look at some different ways we could go about doing this, applying different levels of abstraction:

##### Approach 1 

You could just write out the code needed for each plot. For example:

```{r, eval = FALSE}
library(ggplot2)
linechart1 <- ggplot(dataset1, aes(x = x, y = y, colour = groups)) +
  geom_line()
linechart2 <- ggplot(dataset2, aes(x = x, y = y, colour = second_grouping)) +
  geom_line()
.
.
.
```

After writing our all our code, we'd have 15 different graphs, but a lot of code and a lot of repitition. There's lots of code here that's being shared and every time the same call or section is duplicated, that's twice the code that we might potentially have to debug.

##### Approach 2

A different approach may be to create a different function for each plot. The function would take the dataset and spit out the graphic, a bit like this:

```{r, eval = FALSE}
library(ggplot2)
create_first_linechart <- function(data) {
  ggplot(data, aes(x = column1, y = column2, colour = column3)) +
    geom_line()
}
.
.
.
```

We would eventually have 15 different functions - one for each plot - that we could call to produce all of our graphics. Like this:

```{r, eval = FALSE}
create_first_linechart(dataset1)
create_second_linechart(dataset2)
create_third_linechart(dataset3)
.
.

```

This is a bit cleaner than our first approach because we've separated out our graph-generation logic and it's certainly a step in the right direction, but there's still just as much repetition. Similarly, if there's an error, we're still going to have to debug each function separately.

##### Approach 3

Building on the use of functions, we could create a function that will create all of the line charts and then a separate function that will create all of our bar charts. Our function to create our line charts might look like this:

```{r}
create_linechart <- function(data, x, y, colour) {
  ggplot(data, aes(x = {{ x }}, y = {{ y }}, colour = {{ colour }})) +
    geom_line()
}
create_barchart <- function(data, x, y, colour) {
  ggplot(data, aes(x = {{ x }}, y = {{ y }}, fill = {{ colour }})) +
    geom_col()
}
```

The `{{ }}` brackets let us pass column names from our function to the `aes()` function in `ggplot2`.

Then we'd still have to make 15 separate calls to create our graphics, but we'd have much less code to debug if something went wrong because we're only relying on two functions, not the 15 we were before. Similarly, if we wanted to change something, we'd just have to change the two functions we created and that change would be propogated to all the 15 graphics.

#### Conclusion

Ultimately, we've utilised the concept of abstraction to simplify our code and make debugging easier. The goal when writing code isn't to write it perfectly first time round - in fact, aiming for perfection off the bat can often be detrimental to your work in the long run - but to strike a good balance of simplicity and functionality. There isn't often a 'perfect' amount of abstraction - it just doesn't really work that way, but the more you program and write code in R and the more you think about your problem and what the core goal you're trying to acheive is, the better your code will become.


## Returns

As R is a functional language, functions (and therefore the values that they return) are an important thing to understand as a user. As we learned in the [teacheR](http://arawles.co.uk/teacher) book, when you write a function, the return value will be whatever was last evaluated in that function definition, unless you specified a `return()` call. This "early return" strategy, however, is often a point of contention. Let's have a look at the different points of view:

1. **There shouldn't be any early returns**

At one extreme, there are people who teach that you shouldn't return something early from a function (unless there's an error). In practice, this might look something like this:

```{r, eval = FALSE}
late_bird <- function(x, y, w, add_w = TRUE) {
  ret <- x + y
  if (add_w) {
    ret <- ret + w
  }
  ret
}
```

So what's going on here? Well, we're creating a variable (called `ret`) and always returning that at the end of our function. The value of `ret` changes depending on our parameters, but we always return the value of that `ret` variable.

On the one hand, this makes it clear *which* variable is being returned. But on the other, it's not always clear *what* the value of that variable is. We have to scan down the whole body of the function to see what happens to ret, even though if we set `add_w` to FALSE, nothing happens after the original call.

2. **Never use a common return variable**

At the other end, we could completely avoid return placeholder variables:


```{r, eval = FALSE}
early_bird <- function(x, y, w, add_w = TRUE) {
  if (add_w) {
    return(x + y + w)
  } else {
    return(x + y) 
  }
}
```

Unlike the first example, we only have to read down until the path that we've chosen is finished. For example, if `add_w` is TRUE, then we only need to read down until the first `return()` call and we know what we're going to get. However, this approach would probably be more complicated if we had more than 2 paths or if we're doing complicated actions. Plus, we're duplicating our code here a bit by specifying the `x + y` part in both `return()` calls.

3. **Return early when possible**

And finally, we reach a more middle-of-the-road approach:

```{r, eval = FALSE}
middle_bird <- function(x, y, w, add_w = TRUE) {
  ret <- x + y
  if (add_w) {
    return(ret + w)
  }
  ret 
}
```

Here we use an intermediate variable to avoid duplicating our `x + y` operation, but then we return when we're ready, meaning that someone doesn't have to read to the bottom of the function if they've chosen the `add_w` path.

And herein lies the crux of the issue. Early returns have been a hot topic since the inception of computer programming, and people will continue to have their opinions on what the correct approach should be, so here's mine:

The final return value should follow the "happy" path. That is, if you used the function with its default parameters, it should reach the end and return the final evaluation. Otherwise, you should probably be returning early. This way, when people first glance at the function, they can easily understand the logic and the "default" return value. From there, they can then monitor the edge cases to understand how the return value changes.

So what does this look like exactly? Well, Example 3 is close, but it doesn't return via the final evaluation for the "happy" path. Let's fix that:

```{r, eval = TRUE}
adams_bird <- function(x, y, w, add_w = TRUE) {
  ret <- x + y
  if (!add_w) {
    return(ret)
  }
  ret + w
  
}
```

Now, the "happy" path works its way all the way down to the final expression `ret + w`. But when we change the default value for `add_w` and then deviate from the strict "happy" path, we see an early return call when we're ready.

Of course, this approach won't always be the easiest to understand - for example, if you have a parameter that changes the path at multiple points, then you won't be able to return until later in the function anyway, but to me this approach is the most conducive to readable code.