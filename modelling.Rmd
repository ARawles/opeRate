# Modelling

This chapter is inspired by the simple and concise explanation presented in Hadley Wickham and Garrett Grolemund's [R for Data Science book](https://r4ds.had.co.nz/model-basics.html#model-basics).

A model helps us quantify the relationships between variables in a dataset. By understanding these relationships, we can use these models to better understand the data we have, and to make predictions for unseen data.

A relationship can be quantified by two influences:

* A true pattern
* Random error

The goal of the model is to identify the true patterns from the random error and noise. As an example, let's say you're testing the impact of a new drug on cancer treatment. You have half of your participants take the drug, and the other half take placebo. Then, you want to model the impact of your treatment to see if it actually works. In your data though, you're going to see changes between the groups that are due to your intervention (your drug), but also some changes that are just due to random influences, like individual differences, measurement errors, whatever it may be. The goal of a model is to strip away these random influences and quantify in its purest from, the relationship between your variables of interest.

In reality, you will never perfectly capture a relationship for two reasons:

1. We can never truly strip away the noise.
2. Perfect relationships don't really exist.

But that's not really the point. We will never get things spot on, but models can help us with useful approximations that can inform our decision making and further analysis.

## Types of model

Models types usually fall into "families". These will be a set of models that are underpinned by some common statistics or philosophy. The most common type of model that is taught at beginner level is the linear model. A linear model assumes a relationship that can be represented in the form `y = a + a1x...`, where `a` and `a{n}` represent coefficients (values that will change between models depending on your data).

In this chapter we're going to look exclusively at linear models. In the next chapter though, we'll take a different family of models; neural networks.

### Linear models

A linear model with the formula `y = 8 + (2*x)` might look like this:

```{r, echo = FALSE}

lm_test_data <- data.frame(
  x = 1:10,
  y = (seq(10, 29, by = 2))
)

ggplot(lm_test_data, aes(x = x, y = y)) +
  geom_point(position = "jitter") +
  geom_abline(slope = 2, intercept = 8, linetype = "dashed")
```

A model with different coefficients (`y = 210 - (10*x)`) would look different:

```{r, echo = FALSE}
lm_test_data <- data.frame(
  x = 11:20,
  y = seq(100, 1, by = -10)
)

ggplot(lm_test_data, aes(x = x, y = y)) +
  geom_point(position = "jitter") +
  geom_abline(slope = -10, intercept = 210, linetype = "dashed")

```

Although the value of the coefficients will change, a linear model that uses that formula will always be a straight line.

### Quadratic & Exponential Models

The linear model also extends to quadratic or exponential functions. This is because the 'linear' in linear model refers to the coefficients rather than our regression line being linear.

Quadratic models are expressed by the formula `y = a + (a1*x) + (a2*x^2)`, and allow us to quantify non-linear relationships. For example, a quadratic model with the formula `y = 12000 - (2200*x) + (100*x^2)` might look like this:

```{r, echo = FALSE}
lm_test_data <- data.frame(
  x = 11:20,
  y = (seq(1, 100, by = 10)^2)
)

ggplot(lm_test_data, aes(x = x, y = y)) +
  geom_point(position = "jitter") +
  geom_smooth(method = "lm", formula = y ~ x + I(x^2))
```

Quadratic models allow us to capture relationships between variables that change with different values of x. Real-life examples of relationships like this include:

* population growth
* appreciation of certain commodities.
* disease infection

We'll look at how to identify what type of model is more appropriate for your analysis a bit later.

## Coefficients

In linear modelling, our coefficients are the values that change in our formula to describe our data. In our standard linear model formula (e.g. `y = a + a1*x`), the two coefficients represent the intercept (`a`) and the slope (`a1`). In other words, the `a` represents the value of `y` when `x` is 0 (so where the line would start on the y axis), and `a1` defines how quickly that line goes up or down (its gradient). There will be values for those two coefficients that will produce a better model that others. For example, let's create two models to predict the miles per gallon a car will get based on its weight from the `mtcars` dataset and compare them.

```{r}
mtcars %>%
  ggplot(aes(x = wt, y = mpg)) +
  geom_point() +
  geom_abline(intercept = 45, slope = -6.5, linetype = "dashed") +
  geom_abline(intercept = 35, slope = -5, linetype = "dashed")
```

Here we've created two models with different coefficient values; one where `a = 45` and `a1 = -6.5`, and other where `a = 35` and `a1 = -5`. Both seem to represent the data pretty well, but how can we know which of the coefficients is the 'better' fit? Similarly, we've tried just two sets of values here, how can we compare different combinations of values to determine which model is better? 

## Residuals

When we create a model, there will be a difference between the model's predicted value and each data point. These are the **model residuals**. The smaller the total residual difference, the more accurate the model is. Let's visualise our residuals for one of our two `mtcars` models:


```{r echo=FALSE}
model <- function(coeff, wt) {
  coeff[1] + (wt * coeff[2])
}

predicted_dataset <- mtcars %>%
  dplyr::mutate(prediction = model(c(45, -6.5), wt)) %>%
  dplyr::mutate(residual = mpg - prediction)

predicted_dataset %>%
  ggplot(aes(x = wt, y = mpg)) + 
  geom_point() +
  geom_line(aes(y = prediction), linetype = "dashed") +
  geom_linerange(aes(ymin = prediction, ymax = mpg), colour = "red")
#geom_abline(slope = 21, intercept = -42500)
# Here's where we plot our model
```

To compare different models though, we need to boil down these residuals into a single metric. For this we use the **root-mean-square-error** or RMSE. The calculation for this metric is quite easy - calculate the difference between each data point and the model, square it, average them all and then take the root. Let's write some functions to produce some models and get some predictions, and then compare the RMSE:

```{r}
# Create some models with different coefficients
coefficients <- data.frame(
  id = 1:10,
  a = runif(10, min = 35, max = 40),
  a1 = runif(10, min = -6, max = -5)
)


# Plot the models
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  geom_abline(data = coefficients, aes(intercept = a, slope = a1, colour = as.factor(id)), alpha = 0.5) +
  scale_colour_discrete(name = "Model")

# Create a function to represent our model
model <- function(a, a1, wt) {
  a + (wt * a1)
}

# Create a function to work out the rmse of our model
rmse <- function(a, a1, wt, mpg) {
  # Find the difference between the model value and the actual value
  diff <- model(a, a1, wt) - mpg
  sqrt(mean(diff ^ 2))
}
```

Now we know how to calculate our RMSE, let's apply calculate which of our models has the lowest error:

```{r}
best_model <- coefficients %>%
  # Here we calculate the rmse of each model
  dplyr::mutate(rmse = unlist(purrr::map2(a, a1, rmse, wt = mtcars$wt, mpg = mtcars$mpg))) %>%
  dplyr::filter(rmse == min(rmse))
print(best_model)
```

According to our analysis, model `r best_model$id` has the lowest residual error. Let's see what that model looks like:

```{r}
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  geom_abline(data = best_model, aes(intercept = a, slope = a1, colour = as.factor(id)), alpha = 0.5) +
  scale_colour_discrete(name = "Model")
```

That looks pretty good, but it's not perfect. We've just tried 10 different sets of values and then chosen the best one. This doesn't mean that we've found the absolute best coefficients for our model. To produce the best model possible, we'd need to work out the best values by **optimising** our RMSE function - finding the values that produce the global minimum RMSE value.

That's essentially the process that the `lm()` function in R uses; it automatically finds the best coefficient values by calculating the RMSE for lots of different values and then choosing the best ones.

## `lm()`

Now that we've understood the basics behind a linear model, I can finally reveal that R has a function that will do all of this for us. The `lm()` function in R just needs the dataset and the formula for our model, and works out the coefficients for us, giving us back a model:

```{r}
lm_model <- lm(data = mtcars, mpg ~ wt)
lm_model
# This formula is equivalent to `y = a + (a1*x)`
# where y = mpg and x = wt

```

We can see that the `lm()` function has calculated coefficient values of 37.285 for the intercept, and -5.344 for the slope. We weren't too far off!

To model quadratic relationships with the `lm()` function, our syntax is a little different:

```{r}
lm(data = mtcars, mpg ~ wt + I(wt^2))
# This formula is equivalent to `y = a + (a1*x) + (a2*x^2)
```

**Note:**
The `I()` bit here tells R to treat what we've written (`wt^2`) as one expression, rather than two (`wt` & `2`).

The `lm()` function returns a linear model. We'll take a look at how to evaluate our model in the [Evaluating Models](#evaluating-models) section.

## Multiple Predictors

So far we've focused on the relationship between just two variables. But in reality, you'll be using multiple variables in your model to better predict your dependent variable. When we introduce additional variables, we can look for **simple** or **interaction** effects.

### Simple effects

Simple effects are the additive effects of multiple variables when used to predict a dependent variable. For example, we might want to also introduce the effect of horsepower `hp` on the miles per gallon of the cars in the `mtcars` dataset. To do that, we would just add the `hp` variable to our model:

```{r}
lm(data = mtcars, mpg ~ wt + hp)
```
We now get coefficient values for each of the variables we provided. This tells us the impact of weight and horsepower separately on the `mpg` value. In other words, for every increase of 1 in `wt`, we see a drop of around 4 in `mpg`, and for every increase of 1 in `hp`, we see a drop of around 0.03 in `mpg`. These drops are consistent regardless of the value of either of the variables.

### Interaction effects

Often however, the variables you're introducing into your model won't influence your dependent variable in isolation. Instead, the variables will interact with each other. For example, let's imagine we're modelling the impact of the type of food and condiment its served with on customer satisfaction. Those two variables are going to related - it makes a big difference if you serve mustard with fish and chips compared to roast beef! It wouldn't be appropriate therefore to just identify the relationship between the food and the condiments separately on customer satisfaction. Instead we would to understand how customer satisfaction changes as *both* the food and the condiment changes.

To add interaction effects to our model, we use the `*` operator:

```{r}
lm(data = mtcars, formula = mpg ~ wt * hp)
```

Now we have 3 coefficients:
  * The impact of `wt` alone
  * The impact of `hp` alone
  * The impact of `wt:hp` together
  
We could also write the formula like this `mpg ~ wt + hp + wt:hp`.
  
Now we know how to create models using the `lm()` function, let's look at how we can better understand and evaluate them.

## Evaluating Models

Jumping straight into modelling a dataset you don't understand or creating a model that looks good and then declaring that your work is done can be a dangerous thing. Modelling should be a weapon in your data science arsenal, but it should be handled with care. It's very easy to make an error and end up with a model that is pointless at best and misleading at worst.

To demonstrate this, let's produce 4 models for the each of the sub-datasets in the `anscombe` dataset:

```{r}
tidy_anscombe <- anscombe %>%
  tidyr::pivot_longer(everything(),
                      names_to = c(".value", "dataset"),
                      names_pattern = "(.)(.)"
  )

models <- purrr::map(1:4, ~tidy_anscombe %>% dplyr::filter(dataset == .x) %>% lm(data = ., formula = y ~ x))
models
```


According to our output, we've got 4 very similar models. From that output we might assume that the datasets must be very similar. Let's plot them now:

```{r}
tidy_anscombe %>%
  ggplot(aes(x = x, y  = y)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
  facet_wrap(~dataset)
```

Now that we've looked a little deeper, we can clearly see that these models are inappropriate for 3 of the datasets (2, 3 and 4). The lesson here is that there are multiple tools at your disposal when analysing data, and overeliance on a particular tool without due care and attention can cause issues.

With that in mind, let's look at some ways of evaluating the strength of your model.


### `lm()` Output

Now we've created a model using the `lm()` function, let's understand the output a little bit so we can evaluate our model. When we just print the model, we get the formula we provided, and the coefficients. By using the `summary()` function, we can find out a little more about our model:

```{r}
summary(lm_model)
```

There's a lot to unpack here.

**Residuals**

We've now got a residuals section that shows us the range and quartiles of our residuals. This can be helpful to understand whether your model is consistently over or underestimating

**Coefficients**

We've also now got an expanded view of our coefficients. We've still got our estimates, but now we've also got standard error, t value and then P value entries.

*Standard error*

The standard error represents the average difference between the estimated value and the actual value. So for example, our coefficient estimate for `wt` is roughly -5, meaning that we lose 5 miles per gallon for every increase of 1000 lbs (1 `wt` unit). Our error value of roughly 0.5 tells us that actually, on average, it's between -5.5 and -4.5. If our standard error was 2, then that would suggest that on average, an create in 1 `wt` unit would correspond with a change of between -7 and -3. A lower standard error value usually represents a more accurate model.

*t value*

The t value is then the estimate divided by its error. We can use this value to estimate the strength of our relationship between the two variables; a strong relationship (i.e. a high coefficient value) and a low error will produce a high t value. Conversely, a smaller coefficient value representing a weaker relationship and then a large error value suggesting that we're struggling to accurately predict the value of y will have a low t value.

*P value*

P values are something of a hot topic in statistics so we won't discuss them in length here. Instead, we'll focus on exactly what a P value is - not whether it's a good idea to use one.

A t value is converted to a P value via a T distribution table  based on the degrees of freedom (don't worry about what this is). P values represent how often we would see this same t value if the two variables had absolutely no relationship at all. The larger the t value is (i.e., the stronger the relationship appears to be), the less likely it is that we would observe this relationship purely by chance.
The typically accepted value for considering a relationship to not be the result of chance is 0.05 (i.e. we'd only see these results if there was truly no relationship 1 in 20 times), but that's more an academic rule of thumb. In reality, you shouldn't really rely too heavily on the p value to tell you the strength of your model.

**Residual standard error**

This is the residual standard error of the entire model (as opposed to a single variable). The logic is the same though; it's the average difference between our model's estimate and the actual value. This is almost equivalent to the Residual Mean Standard Error we calculated before, but it's calculated slightly differently.

**Multiple & Adjusted R-squared**

The R-squared value represents how much of the variance in y can be explained by your model. So we've got a Multiple R-Squared value of around 0.7, meaning that 70% of the variance we see in the `mpg` value can be explained by the effect of `wt`. The larger the value the better. But if you ever get an R-Squared value of 1, you've definitely done something wrong.

If you have one variable, then the Multiple R-Squared value is fine to use. If you have more than 1 though, the Adjusted R-Squared value will account for the fact that you're going to explain more variance simply by adding more variables, and so tries to correct for that.

**F Statistic**

The F statistic is a global test for whether any of your coefficients are non-zero (if you have a relationship between any of your variables and the dependent variable). The reason we have this global test is that as you increase the number of variables in our model, you're increasing the likelihood that at least 1 will have a significant relationship with y. We learned before that our rough estimate for considering a 'true' relationship is that we would only observe the t value of that size once every 20 times if there was no relationship. Well then if you include 20 variables, you'll get one significant predictor on average each time. The F value tells you then whether you have at least 1 significant predictor given the number of variables that you tried.

### Residuals

As we've learnt previously, the goal of our model is to reduce the total residual difference between our model and the data. But blindly reducing this value without then inspecting the result can be dangerous. When you're using the `lm()` function, your residuals should be **normally distributed**. That means that (essentially) your model should be overestimating as much as it's underestimating for every value of y. 

Let's take a look at the residual values for our `mtcars` model:

```{r}
mt_residuals <- mtcars %>%
  modelr::add_residuals(model = lm(data = mtcars, formula = mpg ~ wt + hp))

mt_residuals %>%
  ggplot(aes(x = resid)) +
  geom_freqpoly(binwidth = 0.5)
```

Things kind of look okay, but it's tough to know for sure because have so few estimates. Let's take a look at how our residuals change for different `mpg` values:

```{r}
mt_residuals %>%
  ggplot(aes(x = mpg, y = resid)) + 
  geom_point()
```

Here we can see what looks like random noise. This is good because it means that our model isn't performing better for some values than for others. If it is, that means we have a systematic bias in our model.

Alternatively, let's compare this plot with a plot of one of the `anscombe` datasets:

```{r}
tidy_anscombe %>%
  dplyr::filter(dataset == 2) %>%
  modelr::add_residuals(model = models[[2]]) %>%
  ggplot(aes(x = x, y = resid)) +
  geom_point()
```

Here we can see a very clear trend, meaning that our model is not representing the data very well. Instead, it is overestimating and underestimating in a predictable pattern for different values of x. If you see a residual plot like this, you're likely not applying an appropriate model family or type.

## Video Game Sales Model

Now we've created some example models, let's apply what we've learned to our video games dataset.

Let's model the effect of year and genre on video game sales between 1980 and 2000:

```{r}
vg_sales_to_model <- tidy_vg_sales %>%
  dplyr::filter(Year >= 1980 & Year <= 2000,
                Country == "Global") %>%
  dplyr::group_by(Genre, Year) %>%
  dplyr::summarise(Total_Sales = sum(Sales), .groups = "drop") %>%
  dplyr::mutate(Genre = forcats::fct_lump_n(Genre, n = 7, w = Total_Sales)) %>%
  dplyr::filter(Genre != "Other") %>%
  dplyr::group_by(Genre, Year) %>%
  dplyr::summarise(Total_Sales = sum(Total_Sales), .groups = "drop")

vg_sales_to_model %>%
  ggplot(aes(x = Year, y = Total_Sales)) +
  geom_line() +
  facet_wrap(~Genre)
```

There does seem to be a relationship - we're seeing more sales as the years increase - but it might not be equal for the different genres. For this reason, let's create a model that looks at the interaction effects between genre and year:

```{r}
vg_model <- lm(data = vg_sales_to_model, formula = Total_Sales ~ Year * Genre)
```

Let's plot a model for each genre model using `{ggplot2}`:

```{r, warning = FALSE}
vg_sales_to_model %>%
  ggplot(aes(x = Year, y = Total_Sales, colour = Genre)) +
  geom_point() +
  # This will produce the same model as we did with the `lm()` model
  geom_smooth(formula = y ~ x, method = "lm") +
  scale_y_continuous(limits = c(0, NA)) +
  facet_wrap(~Genre)
```

From our plot, it looks as though we're only seeing a different effect of year in a couple of the genres - the role-playing genre seems to have grown at a faster rate than the others, with shooters growing at a slower rate. Let's compare this to our model summary:

```{r}
summary(vg_model)
```

This seems to support what we estimated from the plots - there's definitely a positive effect of year (we're selling more games as the years go by), and we can see that this effect is different for the Role-Playing and Shooter groups. If we look at the estimate values, we can see that the Shooter genre has a negative value and the Role-Playing genre is positive. This also backs up what we estimated from the plots - the popularity of shooter games has grown more slowly than role-playing games.

It's worth noting that when you use a character or factor variable like we have here (`Genre`), the first level of that variable is used as the base for the model. So we can see that there's no `GenreAction` entry in our summary. This is because R has treated that genre as the base (i.e the Intercept value) and then compared the effect of the other levels against that base. So when we say that the popularity of shooter game has grown more slowly, this is compare to the base class (the growth of action games).

Before we call it a day however, let's evaluate our model residuals. If we look at the residuals of the model specifically for the 'Action' genre, they don't look very random:

```{r}
action_genre_model <- lm(data = vg_sales_to_model %>% dplyr::filter(Genre == "Action"), Total_Sales ~ Year)
vg_sales_to_model %>%
  dplyr::filter(Genre == "Action") %>%
  modelr::add_residuals(action_genre_model) %>%
  ggplot(aes(x = Year, y = resid)) +
  geom_point() 
```

Instead, we're overestimating closer to the limits of our year range, and underestimating in the middle. This suggests to me that a quadratic model would be more appropriate. Let's try that:

```{r warning=FALSE}
vg_quad_model <- lm(data = vg_sales_to_model, formula = Total_Sales ~ Year + I(Year^2) + Genre + Genre:Year)


vg_sales_to_model %>%
  ggplot(aes(x = Year, y = Total_Sales, colour = Genre)) +
  geom_point() +
  # This will produce the same model as we did with the `lm()` model
  geom_smooth(formula = y ~ x + I(x^2), method = "lm") +
  scale_y_continuous(limits = c(0, NA)) +
  facet_wrap(~Genre)

summary(vg_quad_model)
```

This seems to have improved things. We now have a higher t value for our Intercept (representing our Action genre), suggesting that we've better captured the relationship between the genre and sales with this new model. When we now look at the interactions, we can see that actually only the shooter genre has a significantly different interaction with Year. In other words, there is no difference in the relationship between Year and Sales between the genres other than if the game is shooter.

Let's check our residuals again to make sure we're not still predictably over or underestimating:

```{r}
qd_action_genre_model <- lm(data = vg_sales_to_model %>% dplyr::filter(Genre == "Action"), Total_Sales ~ Year + I(Year^2))
vg_sales_to_model %>%
  dplyr::filter(Genre == "Action") %>%
  modelr::add_residuals(qd_action_genre_model) %>%
  ggplot(aes(x = Year, y = resid)) +
  geom_point()
```


That looks much more random to me!