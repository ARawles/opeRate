```{r, echo = FALSE}
library(dplyr)
```

# Data Analysis

In this chapter, we're going to go through some typical data science tasks and learn how to do them in R. Later on, we'll look at how you should apply these skills in your projects, and some tips for making your project easier to read and share with others.

For this chapter, we're going to use a  [Kaggle dataset](https://www.kaggle.com/gregorut/videogamesales) that holds information on video game sales:

```{r, echo = FALSE}
print(readr::read_csv("./data/vgsales.csv"), n = 5)
```

## Loading

The first step in any data analysis project you'll undertake is getting at least one dataset. Oftentimes, we have less control over the data we use than we would like; receiving odd Excel spreadsheets or text files or files in a proprietary format or whatever. In this chapter, we'll focus on the more typical data formats (rds, CSV and Excel), but we'll also look at how we might extract data from a web API, which is an increasingly common method for data loading. We'll also look briefly at how you can extract data from a SQL database.

### RDS

If you need to share an R-specific object (like a linear model created with the `lm()` function) or you're certain that the data never needs to be readable in another program, then you utilise the rds format to save and read data.

To load in .rds files, we use the `readRDS()` function, providing the file path of the file we want to read:

```{r, eval = FALSE}
my_data <- readRDS(file = "path/to/file")
```

To save an object to an .rds file, you just need to provide the object you want to save and a file path to save it to:

```{r, eval = FALSE}
saveRDS(my_data, file = "path/to/file")
```

#### Advantages

RDS files are R specific files that contain a serialized version of a specific object. The benefit of R objects is that the object will be preserved in its entirety - you won't lose the column data types when you save it and then load it in again.


#### Disadvantages

RDS files cannot be read natively by other programs. This means that if you're trying to share your dataset and someone wants to open it in, say, Excel, they're going to need to convert it to a different format before they can load it. The RDS format therefore isn't ideal for sharing data outside of the R ecosystem.

### CSV

If I have any say in the data format of the files I need to load in, I usually ask for them to be in CSV format. CSV stands for "comma-separated values" and essentially means that the data is stored as one long text string, with each different value or cell separated by a comma (although you will see CSV files with different separators). So for example, a really simple CSV file may look, in its most base format, like this:

```
name,age,
Dave,35,
Simon,60,
Anna,24,
Patricia,75
```

Benefits of the CSV file over something like an Excel file are largely based around simplicity. CSV files are typically smaller and can only have one sheet, meaning that you won't get confused with multiple spreadsheets. Furthermore, values in CSV files are essentially what you see is what you get. With Excel files, sometimes the value that you see in Excel isn't the value that ends up in R (looking at you dates and datetimes). For these reasons, I would suggest using a separated-value file over an Excel file when you can.

The Kaggle dataset we're going to be using throughout is in CSV format, so we'll use that file as an example.

#### Loading CSV files

Loading CSV files in R is relatively simple. There are base* functions that come with R to load CSV files but there's also a popular package called `readr` which can be used so I'll cover both.

\* They are technically from the `utils` package which comes bundled with R so we'll call it base R.

##### Base R

To load a CSV file using base R, we'll use the `read.csv()` function:

```{r, eval = FALSE}
read.csv(file = "./data/vgsales.csv", header = TRUE, ...)
```

The `file` parameters needs the path to your file as a character string. The `header` parameter is used to tell R whether or not your file has column headers. Our dataset does have headers (i.e. the first row is the column names) so we set that to `TRUE`.

There are lots of other parameters that can be tweaked for the `read.csv()` function, but we won't go through them here.

##### readr

The `readr` package comes with a similar function: `read_csv()`. With the exception of a couple of extra parameters in the `read_csv()` function and potentially some better efficiency, there isn't a massive difference between the two.

Using the `read_csv()` function is simple:

```{r, eval = FALSE}
readr::read_csv(file = "path/to/file", col_names = TRUE)
```

In this function, the `header` parameter is replaced with the `col_names` parameter. The `col_names` parameter is very similar, you can say whether your dataset has column headings, or you can provide a character vector of names to be used as column headers.

Let's load our Kaggle dataset in using the `readr::read_csv()` function:

```{r}
vg_sales <- readr::read_csv("./data/vgsales.csv")
print(vg_sales, n = 5)
```

You can see that when we load in a file using `{readr}` without specifying column types, we get an output showing us exactly how each column has been parsed. This is because CSV is a typeless format, and so data isn't always imported as the type you intended it to be.


To override the default types that `{readr}` has assigned, we can use the `col_types` parameter. This can either be provided using the `cols()` helper function like this:

```{r, eval = FALSE}
readr::read_csv(file = "path/to/file",
                col_names = TRUE,
                col_types = readr::cols(
                  readr::col_character(), readr::col_double(), ... # You need to provide a type for each column
                ),
                ...
)
```

Or, you can provide a compact string with different letters representing different datatypes:

```{r, eval = FALSE}
readr::read_csv(file = "path/to/file",
                col_names = TRUE,
                col_types = "cd",
                ...
)
```

The codes for the different datatypes can be found on the documentation page for the `read_csv()` function (type `?read_csv()`).

Let's use the compact string format to load in the video game dataset again but this time, we want the year to be imported as a number:

```{r, warning = TRUE}
vg_sales <- readr::read_csv("./data/vgsales.csv", col_types = "dccdccddddd")
```
We've got some warnings here because `{readr}` wasn't able to parse some of the values in the Year column as numeric, giving us some NAs. We'll take a look at this further in the [Cleaning](#cleaning) chapter.

Overall, both functions will give you the same result, so just choose whichever function makes most sense to you and has the parameters you need.

#### Advantages

CSV files can be opened in a number of different software packages, making the CSV format a good candidate for sharing data with people who may not also be using R.

#### Disadvantages

In the interests of simplicity, CSV files don't store information on the **type** of the data in each column, meaning that you need to be careful when you're loading data from a CSV file that the column types you end up with are the ones you want.

The CSV format also isn't fully standardised, meaning that you might come across some files that say they're CSV, but generate errors when they're parsed. It's relatively rare to see a CSV file that is so different that it can't be parsed at all, but it's something worth remembering.

#### Other delimited files

Comma-separated value files are just a form of delimited files that use a comma to separate different values. In the wild you might see files separated with all different kinds of symbols, like pipes (|) or tabs (   ). To load in these types of files, use the `readr::read_delim()` function and specify what's being used to separate the values with the `delim` parameter. `readr::read_csv()` basically just wraps `readr::read_delim()` using `delim = ','` anyway, along as you're comfortable loading in CSV files, you should be well equipped to load in any kind of delimited file.

### Excel files

R doesn't have any built-in functions to load Excel files. Instead, you'll need to use a package. One of the more popular packages used to read Excel files is the `readxl` package.

Once you've installed and loaded the `readxl` package. You can use the `read_excel()` function:

```{r, eval = FALSE}
readxl::read_excel(path = "path/to/file", sheet = NULL, range = NULL, ...)
```

Because Excel files are a little bit more complicated than CSV files, you'll notice that there are some extra parameters. Most notably, the sheet and range parameters can be used to define a subset of the entire Excel file to be loaded. By default, both are set to NULL, which will mean that R will load the entirety of the first sheet.

Like the `readr::read_csv()` function, you can specify column names and types using the `col_names` and `col_types` parameters respectively, and also trim your values using `trim_ws`.

#### Advantages

I have something of a personal vendetta against storing everything in Excel spreadsheets because of the terrible way Excel displays data, so I personally don't think there are too many advantages in using Excel files.

You can have more than one sheet maybe? That's all I've got.

#### Disadvantages

The main disadvantage of Excel files for me is that Excel aggressively formats data for the end user. That is, it's difficult to know what value is actually being stored in a cell based on the value that the end user sees in the cell. Dates are a prime example, Excel will show you the date as a date, but will store it as a number with an origin. That alone isn't a sin at all, but combine that with the fact that Excel has [multiple origin dates depending on your Excel version and OS](https://support.microsoft.com/en-us/office/date-systems-in-excel-e7fe7167-48a9-4b96-bb53-5612a800b487#ID0EBBH=Windows), that's a strike in my book.

You could also argue that the xlsx format is a software-specific format, and it kind of is. But because Excel is so ubiquitous now, there are multiple ways of opening and converting xlsx files without ever using Excel, so I don't think that's really a disadvantage.

Overall, if you can send the data in CSV format instead of an Excel file, do that.

### Web-based APIs

Loading static data from text and Excel files is very common. However, an emerging method of data extraction is via web-based APIs. These web-based APIs allow a user to extract datasets from larger repositories using just an internet connection. This allows for access to larger and more dynamic datasets.

#### What are APIs?

API stands for application programming interface. APIs are essentially just a set of functions for interacting with an application or service. For instance, many of the packages that you'll use will essentially just be forms of API; they provide you with functions to interact with an underlying system or service.

For data extraction, we're going to focus more specifically on web-based APIs. These APIs use the HTTP protocols to accept requests and then return the data requested. Whilst there are multiple *methods* that can be implemented in an API to perform different actions, we're going to focus on the `GET` method. That is, we're purely *getting* something from the API rather than trying to change anything that's stored on the server. You can think of the `GET` method as being read-only.

To start with, we're going to look at exactly how you would interact with an API, but then we'll look at the BMRSr package, which I wrote to make interacting with the Balancing Mechanism and Reporting Service easier.

#### Accessing APIs in R

To access a web-based API in R, we're going to need a connection to the internet, something that can use the HTTP protocol (we're going to use the `httr` package) and potentially some log in credentials for the API. In this case, we're going to just use a test API, but in reality, most APIs require that you use some kind of authentication so that they know who's accessing their data.

As previously mentioned, to extract something from the API, you'll be using the `GET` method. The `httr` package makes this super easy by providing a `GET` function. To this function, we'll need to provide a URL. The `GET` function will then send a GET request to that address and return the response. A really simple GET request could be:

```{r, eval = TRUE}
httr::GET(url = "http://google.com")
```

That seems like a really complicated response at first, but when we look at each part, it's quite simple.

* Response
    + This is telling us where we got our response from. In this case, we sent a request to Google, so we got a response from Google.
* Date
    + Fairly self-explanatory - the date and time of the response.
* Status
    + [Status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) give you an indication of how the handling of the request went. 200 means "Success".
* Content-Type
    + This is telling us what type the response is. In this case, the response is just a HTML page, which is exactly what we expect as that's what you get when you type "google.com" into your browser.
* Size
    + This is the size of the response
* Content
    + Below the size, we see the actual response body. In this case, we've been given the html for the google.com page.


As simple as this example was, it didn't really give us anything interesting back, just the Google homepage. So let's use the GET request to get something more interesting.

We're going to access the [jsonplaceholder](https://jsonplaceholder.typicode.com/) website, which provides fake APIs for testing. But for now, imagine that this is something like an Instagram database, holding users and their posts and comments.

The first step in accessing an API is to understand that commands the API is expecting. APIs will have what we call **endpoints**. These are paths that we can use to access a certain dataset. For instance, looking at the website, we can see that there are endpoints for lots of different types of data: posts, comments, albums, photos, todos and users. To access an endpoint, we just need to make sure we're using the correct path. So let's try getting a list of users:

```{r}
httr::GET(url = "https://jsonplaceholder.typicode.com/users")
```

Looking at the content type, we can see that unlike when we sent a request to Google.com, we've got a Content-Type of application/json. JSON is a data structure often used to send data across APIs. We won't go into the structure of it now because R does most of the conversion for us, but if you're interested, there's more info on the JSON structure at [www.json.org](https://www.json.org/json-en.html).

Trying to read raw JSON is hard, but `httr` includes functions to help us get it into a better structure for R. Using the `httr::content()` function, `httr` will automatically read the response content and convert it into the format we ask for (via the `as` parameter). For now, we're going to leave the `at` parameter as 'NULL' which guesses the best format for us.

```{r}
response <- httr::GET(url = "https://jsonplaceholder.typicode.com/users")
content <- httr::content(response)
head(content, 1) # we'll just look at the first entry for presentation sake
```

We can see that R has taken the response and turned it into a list for us. From here, we can then start our analysis.

In many cases however, you won't want a complete list. Instead, you'll want to provide some parameters to limit the data you get back from your endpoint. Most APIs will have a way of doing this. For example, reading the jsonplaceholder website, we can see that we can get all the posts for a specific user by appending the url with "?userId=x". This section of the URL (things after a ?) are called the query part of the URL. So let's try getting all of the posts for the user with ID 1:

```{r}
response <- httr::GET(url = "https://jsonplaceholder.typicode.com/posts?userId=1")
content <- httr::content(response)
head(content, 1) # we'll just look at the first entry for presentation sake

```

Whilst the parameters here are pretty simple, you will come across APIs that accept multiple parameters, making data extraction from an API a very powerful tool.

#### BMRSr

As easy as the above was, interacting with APIs that have several parameters and complicated URLs can get confusing. To this end, many people create packages in R that act as wrappers for various APIs. These packages will then provide you with functions that will automatically create the request, send it and receive and parse the content. You can kind of think about it as an API for an API!

This is what I did for the Balancing Mechanism Reporting Service ([BMRS](https://bmreports.com/)) API. BMRS provides a massive amount of energy-related data, but creating the correct URLs and dealing with the response can be tricky. The BMRSr package that I wrote was designed to help with that.

We'll now go through a quick demo of the BMRSr package. If you're not too bothered about this part, feel free to skip to the next section.

If you're interested, there are a couple of things you'll need:

* The BMRSr package installed
* A free BMRS API key that can be retrieved from the [ELEXON portal](https://www.elexonportal.co.uk/).

Once you've got those two prerequisites, using BMRSr should be quite easy. The main function in the BMRSr package is the `full_request()` function, which will create your URL, send the request, and parse the response depending on your parameters. To do this however, the `full_request()` function needs some parameters:

* `data_item`
    + A data item to retrieve. The BMRS platform holds lots of datasets, and so we need to specify which one we want to retrieve.
* `api_key`
    + Our API_key that we got from the Elexon portal
* parameters
    + Depending on which data_item you chose, you'll need to provide some parameters to filter the data
* `service_type`
    + What format you want the data returned in: values are XML or CSV.

So what parameters do I need? Well, the easiest way to find out is to use the `get_parameters()` function. This will return all of the parameters that can be provided to the `full_request()`.

Let's do an example. Say I want to return data for the B1620 data item, which shows us aggregated generation output per type. So, the first step is to know what parameters I can provide using the `get_parameters()` function:

```{r}
BMRSr::get_parameters("B1620")
```

This tells me that I can provide two parameters in my request - the date and the settlement period. Using this information in my `full_request()` function...

```{r, eval = FALSE}
bmrs_data <- BMRSr::full_request(data_item = "B1620",
                                 api_key = "put_your_API_key_here",
                                 service_type = "csv",
                                 settlement_date = "01/11/2019",
                                 period = "*") # From reading the API manual,
# I know that this returns all periods
head(bmrs_data, 2)
```


```{r, echo = FALSE}
head(
  readRDS("data/example_BMRSr.rds"),
  2)
```

And there we have it, we've retrieved a energy-related dataset from an API using the BMRSr package. There are roughly 101 data items available on BMRS so there's a massive amount of data there for those who want to access it.

### Databases

In the corporate world or when you're dealing with larger systems, you'll often have some form of database that stores all of the system data. This database can function as a great repository of data for your analyses, and by utilising a live connection to the database, we can easily update our analyses in the future.

In this section, we'll really only cover the basics of how R can interact with an SQL database, so don't feel as though you need to be a data engineer expert to understand this section. As long as you know the tiniest bit of SQL, you should be fine.

#### Database Management Systems and ODBC

There are lots of different types of database managements systems (DBMS) such as SQL Server, MySQL, MariaDB, and so on. These DBMSs allow us to create and maintain databases.

When we connect to our database, we do so through our DBMS, but all of them have slightly different implementations. This means that if we developed a package to connect to one type of database (like MySQL), we'd have to create an entirely different package to do the same thing with a MariaDB database. This is where the Open Database Connectivity (ODBC) standard comes in. The ODBC standard allows is to interface with databases that use different DBMS systems using a common API. This means that we could swap out our SQL Server database with a MySQL one, and we wouldn't need to make too many changes. The ODBC standard is implemented via a driver; this driver functions as an interface layer between our application (R) and the database.

So when we connect to a database with R we go in this order:

R -> ODBC Driver -> DBMS

R tells the ODBC driver to run a query, and then the ODBC driver converts that request to one that can be interpreted by the DBMS that it was built for. This means that as long as we've got the appropriate ODBC driver for our DBMS, we can use (basically) the same R code to interact with any kind of ODBC-compliant database.

So to connect to our database, we're going to need 2 things:

* The `odbc` R package
    + This provides us with the R functions to create our connections and run our queries as so on.
    + Think of the `odbc` package as a set of tools for interacting with any ODBC database; it knows how to interface with the ODBC drivers, not the DBMS.
* The ODBC driver for our database
    + This is the actual driver used by the `odbc` package that communicates with the database.
    + This is the implementation of the ODBC standard for the database we're using.
    
#### Connecting with the `odbc` package

For this example, let's say that we're using an SQL Server database, and so we've got the ODBC Driver 17 for SQL Server installed.

First, let's make sure we've got the driver detected:

```{r, eval = FALSE}
odbc::odbcListDrivers()
```

```{r, echo = FALSE}
print(structure(list(name = c("ODBC Driver 17 for SQL Server", "ODBC Driver 17 for SQL Server", 
"ODBC Driver 17 for SQL Server"), attribute = c("Description", 
"Driver", "UsageCount"), value = c("Microsoft ODBC Driver 17 for SQL Server", 
"/opt/microsoft/msodbcsql17/lib64/libmsodbcsql-17.7.so.2.1", 
"1")), class = "data.frame", row.names = c(NA, -3L)))
```


We can see our driver has been detected. Now we can use the `odbc::dbConnect()` function to create a connection to our database that we can then use to run queries:

```{r, eval = FALSE}
my_connection <- odbc::dbConnect(drv = odbc::odbc(),
                                 driver = "ODBC Driver 17 for SQL Server",
                                 server = "SQLDATABASESERVER" # You'll need to change this to your server
                                 database = "SQLDATABASE") # You'll need to change this to your database
```

With the `drv` parameter we're specifying the type of driver that we're using as a `DBIDriver` object. Because we're using an ODBC driver, we can use the `odbc::odbc()` function that returns a `DBIDriver` object. The `driver` parameter expects a character string of the actual driver we're going to use (not the type), so that's where we enter the name we saw when we ran the `odbc::odbcListDrivers()` function. The `server` is the server that your database is being on hosted on and the `database` parameter is the database on the server that you want to connect to.

The `odbc` will use this information to create a [connection string](https://www.connectionstrings.com/) that it will then use to try and connect to the database.

**Note:**
You'll want to make sure you assign your connection to something because we'll be passing the connection object to some other functions soon. 


##### Authentication

You might also have to specify some credentials when you try and connect to your database. To provide a username and password, just use the `uid` and `pwd` parameters. If your database supports it, you can use Windows Authentication, meaning that you don't need to provide an explicit username and password - the database will use your Windows account instead. To force the driver to try and use this type of authentication, you can add a `Trusted_Connection` parameter to the function call and set the value to `"Yes"` (not `TRUE` or `"TRUE"`). This parameter is then added to the connection string via the `...` argument of the `odbc::dbConnect()` function.

##### DSNs

Instead of specifying this information via the `odbc::dbConnect()` function, you can also create a Data Source Name (DSN) entry. This contains essentially the same information as we provided (the server location, the database, access credentials and so on) but then allows us to just use the `dsn` parameter of the `odbc::dbConnect()` function.

A DSN entry might look like this:

```
[MyDatabase]
Driver = /opt/microsoft/msodbcsql17/lib64/libmsodbcsql-17.7.so.2.1
Server = SQLDATABASESERVER
Database = SQLDATABASE
Port = 1234

```

This can be particularly useful if you're working in more than one environment, where you might want to connect to the same database but the connection string is going to be different for each environment. Instead, you can create a Data Source Name entry with the same name but with different specifications, and then use the same `odbc::dbConnect(drv = odbc::odbc(), dsn = "MyDatabase")` call in both.

I won't go into exactly how to add DSNs here because it depends on your driver and your OS, but the process is pretty simple once you've found the right information for your setup.

#### Querying the database

Now we've got our connection (`my_connection`), we can send queries to the database. To send queries and get back the results in one step, we can use the `odbc::dbGetQuery()` function, passing the connection and then the string containing the SQL we want to execute.

```{r, eval = FALSE}
odbc::dbGetQuery(my_connection, "select top 1 Id from dbo.ExampleTable")
```

```{r, echo = FALSE}
data.frame(Id = 1)
```

We then get back the data as a normal R data.frame. R will deal with the data type conversion, but the exact conversion is dependent on the driver. Luckily, Microsoft provides a [complete breakdown](https://docs.microsoft.com/en-us/sql/machine-learning/r/r-libraries-and-data-types?view=sql-server-ver15) of how data types are mapped between on R and SQL Server on their website.


## Cleaning

Now you should have some data loaded into R. Most of the time however, you'll have messy datasets with odd columns and missing data points that you'll need to deal with before you can actually do any meaningful analysis: you'll need to **clean** your data.

Here are some of the more common operations that you'll be doing when it comes to data cleaning:

* Filtering
* Removing/replacing missing values
* Changing column types

Let's look at how we might do these tasks in R using the `datasets::airquality` dataset as an example.

```{r}
head(datasets::airquality, 10)
```

### Filtering

In some cases, you'll only want a subset of the data that's been provided to you. To filter the dataset down, use the `dplyr::filter()` function and provide your criteria:

```{r}
datasets::airquality %>%
  dplyr::filter(Month == 5) %>%
  head(5)
```

You can utilise the `&` and `|` operators (`and` and `or` respectively) to create more complicated criteria. You can also separate your expressions with commas, which is equivalent to `&`:

```{r}
datasets::airquality %>%
  dplyr::filter(Month == 5 | Month == 6) %>%
  head(2)

datasets::airquality %>%
  dplyr::filter(Month == 5, Temp > 70) %>%
  # equivalent to dplyr::filter(Month == 5 & Temp > 70)
  head(2)
```

### Missing values

Missing values are common in data science - data collection is often imperfect and so you'll end up with observations or data-points missing. Firstly, you need to decide what you're going to do with those.

The easiest approach to just to remove them, and we can do that with the `dplyr::filter()` and the `is.na()` functions. If you remember in the [Loading](#loading) chapter, `{readr}` had trouble parsing some of the values in our Year column to numeric, and so gave us some NAs. Let's remove those rows using this approach:

```{r}
## To remove rows with NA in one column
clean_vg_sales <- vg_sales %>% 
  dplyr::filter(!is.na(Year))
print(clean_vg_sales, n = 5)


## To remove rows with NA in any column
vg_sales %>% 
  dplyr::filter(dplyr::across(dplyr::everything(), ~!is.na(.x))) %>%
  print(n = 5)
```


Another approach to replace them with either the average for that column or with the closest neighbour value (this works better with time series data).

To replace with the nearest value, we can use the `tidyr::fill()` function. That approach wouldn't be appropriate for our video games example so let's use the `datasets::airquality` dataset instead:

```{r}
## Fill a single column
datasets::airquality %>%
  tidyr::fill(Ozone, .direction = "downup") %>%
  head(5)

datasets::airquality %>%
  tidyr::fill(dplyr::everything(), .direction = "downup") %>%
  head(5)


```

To replace them with the mean, we can either use a package like `zoo`, or we can use the `tidyverse` packages and our own function:

```{r}
replace_with_mean <- function(x) {
  replace(x, is.na(x), mean(x, na.rm = TRUE))
}

datasets::airquality %>%
  # The mutate function creates new columns or overwrites existing ones
  dplyr::mutate(dplyr::across(dplyr::everything(), replace_with_mean)) %>%
  head(5)
```


A word of warning here, however. If you're doing complex modelling or very sensitive analyses, filling values like this can be misleading at best. Always think about the best approach for your specific project and what the repercussions of filling empty values might be.

### Changing column types

When you import data into R, sometimes the type of the column doesn't match what you want it to be. One way of tackling this is to define your column types when you import the data (as we looked at before), but it's also perfectly acceptable to change the column type after the import.

Probably the most common conversion is from a character string to a date. For this example, we're just going to use some test data:

```{r}

bad_tibble <- tibble::tribble(~bad_date, ~value,
                              "2012/01/01", 100,
                              "2014/06/01", 200)

bad_tibble %>%
  dplyr::mutate(good_date = as.Date(bad_date, format = "%Y/%m/%d"))

```

Essentially, all you need to do is wrap your conversion function (e.g. `as.Date()`, `as.character()`) in a `dplyr::mutate()` call and you should be able to change your columns to whatever you need. To replace the original column, make sure you give it the same name when using `dplyr::mutate()`:

```{r}
bad_tibble %>%
  # This will replace the bad_date column rather than creating a new column
  dplyr::mutate(bad_date = as.Date(bad_date, format = "%Y/%m/%d"))
```

## Tidying

Now that you know your dataset is clean, you'll want to get it into a format that is amenable to your analysis. This is the **tidying** stage. To better understand what "tidy" data is and why we're trying to get our data into this format, make sure you've read the [tidyverse](#tidyverse) chapter.

### Pivoting columns

There two different forms of data:
  * Long
  * Wide
  
Tidy data takes this long form - as you add more observations, you'll add more rows.
Untidy data often takes the wide form - as you add more observations, you'll add more columns.

Let's look at an example. Imagine the following dataset showing the average temperature each year and in each month:

```{r, echo = FALSE}
column_example <- data.frame(Year = c("2012", "2013", "2014"),
                             Month1 = c(10, 20, 30),
                             Month2 = c(20, 30, 40),
                             Month3 = c(30, 40, 50))
column_example_long <- tidyr::pivot_longer(column_example,
                                           cols = tidyselect::starts_with("Month"),
                                           names_to = "Month",
                                           values_to = "Temperature",
                                           names_transform = list(Month = readr::parse_number))
```


```{r}
head(column_example, 3)
```

As you can see, the more months we add, the wider the data is going to get. How would the same data look in the long (tidy) format?

```{r}
head(column_example_long, 3)
```

Now we've got the month as a variable, the more data we add, the longer the dataset is going to get.

**Note:**
One thing to keep in mind is that long data does not always mean tidy, but wide data cann never be tidy. It's a subtle distinction but it's important to remember.

Wide data is very common, usually because it's slightly easier to read or to enter (particularly if you're using Excel), but it's much harder to work with in R. Let's look at how we might convert wide data to tidy-er long data.

### Pivoting columns to rows (longer)

Wide data will always be untidy because it breaks the first rule of tidy data: each column should be a separate variable. Let's revisit our wide example:

```{r}
head(column_example, 3)
```

In this example dataset, we've clearly broken the first rule of tidy data: each column is not a unique variable. Instead, we've got the same variable (temperature) in different columns for each month.

How can we convert this example dataset to the tidy, long format? Well, we can utilise the concept of pivoting to transform those month columns into two columns; one with the value (the temperature) and the other with the month. In this case, we need to use the `pivot_longer()` function from the `{tidyr}` package, because we want to pivot from the wide format to the long format.

To do this in the simplest way, we just need to tell the function which columns we want to pivot:

```{r}
tidyr::pivot_longer(column_example, cols = c(Month1, Month2, Month3))
```

This is a good start. Now we've converted to long format, we're abiding by the three rules and so we've got a tidy dataset! But there's definitely some improvements to be done. Firstly, "name" and "value" aren't the best names we could come up with for these columns, so we should probably use some new ones. To do this, we just need to provide new names to the `names_to` and `values_to` parameters:

```{r}
tidyr::pivot_longer(column_example,
                    cols = c(Month1, Month2, Month3),
                    names_to = "Month",
                    values_to = "Temperature")
```


Secondly, our Month column has the months as character string and prepended with "Month". Instead, we should store just the month number as a number (or maybe as a factor but we'll go with number). To do that, we can utilise the `names_prefix` parameter to remove matching text from the start of each variable name:

```{r}
tidyr::pivot_longer(column_example,
                    cols = c(Month1, Month2, Month3),
                    names_to = "Month",
                    values_to = "Temperature",
                    names_prefix = "(Month)") # Remove text matching "Month" exactly
```

That's pretty good, but that column is still a character column. We can fix that by using the `names_transform` parameter with a function that we want to use to transform the column:

```{r}
tidyr::pivot_longer(column_example,
                    cols = c(Month1, Month2, Month3),
                    names_to = "Month",
                    values_to = "Temperature",
                    names_transform = list(Month = readr::parse_number))

```

That's better. But what happens now if we get sent the same dataset but with all 12 months worth of data? We'd have to add those extra columns to our `cols` vector. Instead, we can use some `tidyselect` syntax to choose columns based on their features, like how they start:


```{r}
tidy_column_example <- tidyr::pivot_longer(column_example,
                                           cols = tidyselect::starts_with("Month"),
                                           names_to = "Month",
                                           values_to = "Temperature",
                                           names_transform = list(Month = readr::parse_number))
print(tidy_column_example, n = 3)
```

Now our code will work regardless of how many months there are.

This is just one example of how we could convert one form of 'messy' data to the tidy format. In the wild you'll get data that violates any of the three cardinal rules in many different ways, and we just don't have the space to go through it here. If you do need how to convert some of the other forms of messy data, then the `{tidyr}` package has a number of [vignettes](https://tidyr.tidyverse.org/articles/index.html) outlining many different ways to use the tidying tools it provides.

### Pivoting rows to columns (wider)

Most of the time you should be going from wide to long, but we'll go through how to do the reverse for the occasions where it's required. To transform the data to the wide format, we use the `pivot_wider()` function. At the simplest level, we just need to provide where the name comes from and where the values come from with the `names_from` and `values_from` parameters respectively:

```{r}
tidyr::pivot_wider(tidy_column_example, names_from = Month, values_from = Temperature)
```

To get our data more like we started, we can use the `names_prefix` argument to prepend the column names with a string:

```{r}
tidyr::pivot_wider(tidy_column_example, names_from = Month, values_from = Temperature, names_prefix = "Month")
```

## Grouping

Before we look at adding new columns and summarising our data, we need to understand the concept of grouping our data.

When a data frame has one or more groups, any function that's applied (by `dplyr::mutate()` or `dplyr::summarise()`) is applied for each combination of those groups. So say we have a dataset like this:

```{r, echo = FALSE}
ungrpd <- data.frame(Year = c("2012", "2012", "2012", "2012", "2012", "2012", "2013", "2013", "2013", "2013", "2013", "2013"),
                      Month = c(1,1,2,2,3,3,1,1,2,2,3,3),
                      Group = c("A", "B", "A", "B", "A", "B", "A", "B", "A", "B", "A", "B"),
                      Value = c(10,20,30,15,25,35,12,22,32,40,50, 60))
```

```{r}
head(ungrpd, 5)
```
If we grouped by Year or by Month, then any function we applied would be applied separately to each value of those groups. So say we wanted get the total values for each Year, we could group by the `Year` column and then just sum the `Value` column. The same logic applies for if we wanted to get the total for each year *and* month; we could group by the `Year` and `Month` and sum the `Value` column and we'd then get a value for each distinct year and month.

To group a dataset, we use the `dplyr::group_by()` function:

```{r}
grpd <- dplyr::group_by(ungrpd, Year)
```

The `dplyr::group_by()` function returns the same dataset but now grouped by the variables you provided. At first, it might not seem as though anything happened - if we print the dataset we still get basically the same output...

```{r}
print(grpd, n = 5)
```

Except now we have a new entry `Groups:`. This tells us that the dataset has been grouped and by what variables. This means that any subsequent summarisation or mutation we do will be done relative those groups.

To test whether a dataset has been grouped, you can use the `dplyr::is_grouped_df()`, and to ungroup a dataset, just use `dplyr::ungroup()`:

```{r}
dplyr::is_grouped_df(grpd)
dplyr::is_grouped_df(ungrpd)
print(dplyr::ungroup(grpd), n = 5)
```


## Mutating

Sometimes you'll want to add new columns to your data. Most of the time, these will be calculated columns that can be created based on one or more of the other columns in the dataset. To create new columns, we use the `dplyr::mutate()` function.

First off, let's add something simple; a column that calculates the proportion of EU sales compared to global sales. All we need to do is provide the name of our new column to the `dplyr::mutate()` function and how our new column should be calculated:

```{r}
clean_vg_sales %>%
  dplyr::mutate(EU_Proportion = EU_Sales/Global_Sales) %>%
  dplyr::select(Rank, Name, Platform, Year, Genre, Publisher, EU_Proportion) %>%
  print(n = 5)
```

As you can see, we've created a new column called `EU_Proportion` and we've calculated by dividing the number of EU sales by the total number of sales.

While this is certainly a powerful tool, we're not really changing the world here. However, we can create more complicated columns by leveraging the concept of applying our function to [groups](#grouping).

When our dataset is grouped, our `mutate()` call will be evaluated for each permutation of the provided groups. Let's look at an example of creating a cumulative sum for each Publisher:

```{r, warning = FALSE}
clean_vg_sales %>%
  dplyr::group_by(Publisher) %>%
  dplyr::arrange(as.numeric(Year)) %>% # We need to order from earliest date to latest
  dplyr::mutate(Cumulative_Sales = cumsum(Global_Sales)) %>%
  dplyr::filter(Publisher %in% c("Atari", "Activision")) %>% # Let's just look at Atari & Activision
  print(n = 5)
```

Now we've got a new column showing the total number of sales for each Publisher up to each year. What `{dplyr}` has done is essentially filtered the data down a specific publisher, created a cumulative sum of the `Global_Sales` column, and then done that for each publisher and stitched it back together.

## Summarising

As we saw previously, the base videa game sales dataset has nearly 17,000 entries! That's a lot of data, and we're not really going to able to extract or show any real insights from this data without summarising it to some degree. What we want to do is summarise the data to some degree. We lose some of the granularity of the data but it allows is to understand the data a bit better and identify patterns.

Summarisation uses exactly the same [grouping](#grouping) concept as mutatation does; by grouping the data we change the scope of each application of the function to be limited to that group. 

Let's go through an example. Let's say that we want to get the total sales for each publisher in each year. For now, we don't care about genre or platform. To do this, we'll need to group by Publisher and by Year:

```{r, eval = FALSE}
clean_vg_sales %>%
  dplyr::group_by(Publisher, Year)
```

And then we'll want to sum up the `Global_Sales` column to get our totals:

```{r}
smmrsd_vg_sales <- clean_vg_sales %>%
  dplyr::group_by(Publisher, Year) %>%
  dplyr::summarise(Total_Global_Sales = sum(Global_Sales), .groups = "drop")
# The .groups = 'drop' parameter will automatically remove the groups
print(smmrsd_vg_sales, n = 5)
```

Now we can see that the number of rows has reduced down to just under 2,000, and we've got the total global sales for each of our publishers, and for each year. `{dplyr}` has looked through each publisher and each year that publisher has at least 1 game, and added up the total number of sales, then moved onto the next Publisher/Year combination.

While we're here, let's lump together some of the less popular publishers using the `forcats::fct_lump_n()` function:

```{r}
smmrsd_vg_sales <- clean_vg_sales %>%
  dplyr::group_by(Publisher, Year) %>%
  dplyr::summarise(Total_Global_Sales = sum(Global_Sales), .groups = "drop") %>%
  # Now we want to change the value of the Publisher column so that if the publisher wasn't in the top 5,
  # change it to 'Other'
  dplyr::mutate(Publisher = forcats::fct_lump_n(Publisher, n = 5, w = Total_Global_Sales)) %>%
  dplyr::group_by(Publisher, Year) %>%
  dplyr::summarise(Total_Global_Sales = sum(Total_Global_Sales), .groups = "drop")
# The .groups = 'drop' parameter will automatically remove the groups
print(smmrsd_vg_sales, n = 5)
```

In this case we've used the `sum()` function, but any kind of summary function can be used:

```{r}
clean_vg_sales %>%
  dplyr::group_by(Publisher, Year) %>%
  dplyr::summarise(Total_Global_Sales = sum(Global_Sales),
                   Average_Global_Sales = mean(Global_Sales),
                   .groups = "drop")
```

Because `dplyr::mutate()` and `dplyr::summarise()` both use a similar syntax and both function on grouped datasets, using the correct function when starting out can be tough. The best way to remember which one to use is to ask "How many rows am I expecting back from this?". If the answer is fewer than you've got now, you'll want the `summarise()` function. Otherwise you're looking at `mutate()`.

## Plotting

Now that you've got your data summarised to a suitable level, you might want to create some graphics to help gain insight into the trends and patterns in the data. For this section, we're going to rely on the `{ggplot2}` package from the tidyverse. This is arguably the most advanced plotting package available for R. R does provide plotting functions without the use of packages, but we're going to focus on `{ggplot2}` here.

### ggplot2

`{ggplot2}` is based on the concept of a [Grammar of Graphics](https://www.springer.com/in/book/9780387245447). This is the concept that, like language, graphics have a grammar that allow us to describe their components. 

This concept was originally proposed by Leland Wilkinson, but has been adopted heavily in the `{ggplot2}` philosophy, with Hadley Wickham publishing a paper titled ['A Layered Grammer of Graphics'](http://vita.had.co.nz/papers/layered-grammar.html) that outlines his proposal for a layered grammar and how it's implemented in `{ggplot2}`. We'll cover the real basics of `{ggplot2}` here but I'd recommend reading the paper and reading the `{ggplot2}` [documentation](https://ggplot2.tidyverse.org/) if you're interested in gaining a deeper understanding.

#### Components

At the core of the Grammar of Graphics philosophy is the idea that plots are defined by their components. For example, two scatterplots could be extremely different, even though we'd both call them scatterplots. Instead, the two graphics are better defined by the components that make them up.

When we create a plot using `{ggplot2}` we build it up by creating and combining these components.

The main components that make up a plot are:

* Data and aesthetics mappings
* Scales
* Geometrics objects (or geoms)
* Facets

These sound really scary, but they're actually super simple. Let's look at each one.

##### Data and aesthetic mappings

Every plot is a representation of some data. Therefore, to have a graphic, you need some data. That's simple enough.

But which parts of the data should be shown on the plot and where? This is what our aesthetic mappings represent. We might have a dataset with two variables, `z` and `w`. Our mapping may be then that we want the `z` variable on the x axis of the plot, and the `w` variable on the y axis. We can also utilise other mappings like colour and size.

To define our dataset and our aesthetic mappings using `{ggplot2}`, we use the `ggplot()` and `aes()` functions. Let's see an example using our video game sales experiment from the previous chapters:

```{r}
library(ggplot2)

ggplot(smmrsd_vg_sales, mapping = aes(x = Year, y = Total_Global_Sales, colour = Publisher))
```

Here we've defined that we want to show the `Year` on the X axis, the `Total_Global_Sales` on the Y, and we want to map the `Publisher` to the colour of whatever we show on the plot.

As part of our mapping, we can see that `{ggplot2}` has also provided appropriate scales for our variables. It's provided a numeric scale with an appropriate range for the X and Y axis. We can't see it yet, but it's also assigned a scale to our colour aesthetic, mapping colours to values in the Publisher column. We'll look at manually specifying and customising the scales later.

Although we've got our scales, and we can see that `{ggplot2}` clearly knows which variable we want on each axis, but there's nothing on the graph at the moment... This is because we now need to define our geoms.

##### Geometric objects (geoms)

Our geometric objects will be the object that's actually shown on the plot (e.g. bars, points, lines, and so on). To add our geometric object, we use the `geom_...` functions. Our geom will then inherit the mapping from our `ggplot()` call. If we wanted to add additional mappings, the `geom_...` functions also allow for a `mapping` parameter where additional mappings can be provided.

We want lines for our example, so we use the `geom_line()` function. To add our geom component, we just need to add it to our plot so far using the `+` operator:

```{r}
ggplot(smmrsd_vg_sales, mapping = aes(x = Year, y = Total_Global_Sales, colour = Publisher)) +
  geom_line()
```

This is equivalent to:

```{r, eval = FALSE}
ggplot(smmrsd_vg_sales) +
  geom_line(mapping = aes(x = Year, y = Total_Global_Sales, colour = Publisher))
```


Different geoms also allow for different mappings. For example, the line geom allows you to specify the `linetype` aesthetic that changes the way the line is drawn (solid, dashed, etc.) depending on the value of your variable. This aesthetic would mean nothing for the point geom however, which instead has an aesthetic called `shape`. To see which aesthetics are supported by which geoms, look at the documentation for the geom function you want to use (e.g `?geom_point`).

#### Stats

For a data point to be shown on the plot, it can go through a statistical transformation or 'stat' which is set via the `stat` parameter of our `geom_...` function. We can see that when we call `geom_point()` the default value for the `stat` parameter is 'identity'. This means that `{ggplot2}` performs no stat transformation on the data before it plots it. 

We can change this however, so that some form of stat is done on the data before it's plotted. For example, if we set the `stat` parameter to 'smooth', the values are transformed by a smoothing function (the function can be changed via the `method` parameter):

```{r}
ggplot(smmrsd_vg_sales) +
  geom_line(mapping = aes(x = Year, y = Total_Global_Sales, colour = Publisher), stat = "smooth")
```

This call is equivalent to using the `geom_smooth()` function.

#### Layers

Together, the data, mapping, stat and geom components form a **layer**. A plot can be made up of multiple layers. For example, let's show the same data but overlay a regression line that uses a different stat and a different mapping:

```{r}
ggplot(smmrsd_vg_sales, mapping = aes(x = Year, y = Total_Global_Sales)) +
  geom_line(mapping = aes(colour = Publisher)) +
  geom_smooth(method = "loess", formula = y ~ x)
```

We've now got two layers to this plot, with slightly different mappings and stats. The colours lines are split by Publisher, but the regression line does not use that colour mapping, meaning that we have one line for all the Publishers. The coloured lines also have no stat transformation, where the regression line does use a stat transformation - it calculates a smooth conditional mean of y given x (`y ~ x`). In fact, we could even change the dataset for the two layers if we wanted to.

In summary, a layer is made up of:

* A dataset and aesthetic mapping (aes)
* A statistical transformation (stat)
* A geometric object (geom)

And plots can be made of up one or more layers.

##### Facet

In our example, we've shown the Publisher column as a mapping to the colour aesthetic, but we could also use faceting. Faceting splits the original dataset, plots the data separately and then combines it into a single panel. To facet our plot, we use the `facet_wrap()` or `facet_grid()` functions:

```{r}
ggplot(smmrsd_vg_sales, mapping = aes(x = Year, y = Total_Global_Sales, colour = Publisher)) +
  geom_line() +
  facet_wrap(~Publisher)
```

Facets are not features of layers, but instead are universal to the plot.

#### Scales

`{ggplot2}` has created default scales for the variables in our mapping, but we can alter them manually. To do so, we just use the accompanying `scale_{aesthetic}_{type}` function. So for our x axis, we're scaling our x mapping and we want a continuous, numeric scale so we would use the `scale_x_continous()` function. Similarly, for our y axis we'd use `scale_y_continuous()`:

```{r, warning = FALSE}
ggplot(smmrsd_vg_sales, mapping = aes(x = Year, y = Total_Global_Sales)) +
  geom_line(mapping = aes(colour = Publisher)) +
  geom_smooth(method = "loess", formula = y ~ x) +
  scale_x_continuous(name = "Year") +
  # Let's change the limits to stop at 0
  scale_y_continuous(name = "Global Sales (Millions)", limits = c(0, NA))
```

To change the colour scale, we would use the `scale_colour_brewer/hue()` function. These two functions take a slightly different approach in how they assign colours to the levels of the variable.


```{r}
ggplot(smmrsd_vg_sales, mapping = aes(x = Year, y = Total_Global_Sales)) +
  geom_line(mapping = aes(colour = Publisher)) +
  geom_smooth(method = "loess", formula = y ~ x) +
  scale_x_continuous(name = "Year") +
  scale_y_continuous(name = "Global Sales (Millions)", limits = c(0, NA)) +
  scale_colour_brewer(name = "Publisher", palette = "BrBG")
```

```{r}
ggplot(smmrsd_vg_sales, mapping = aes(x = Year, y = Total_Global_Sales)) +
  geom_line(mapping = aes(colour = Publisher)) +
  geom_smooth(method = "loess", formula = y ~ x) +
  scale_x_continuous(name = "Year") +
  scale_y_continuous(name = "Global Sales (Millions)", limits = c(0, NA)) +
  scale_colour_hue(name = "Publisher", l = 40, c = 30)
```

Scales, like facets, are not components of layers. They will be constant across all layers in the plot.

## Modelling

* `lm()`
* Other modelling packages
* GPU-accelerating modelling